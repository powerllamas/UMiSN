\documentclass{article}
\usepackage{polski} %moze wymagac dokonfigurowania latexa, ale jest lepszy niż standardowy babel'owy [polish] 
\usepackage[utf8]{inputenc} 
\usepackage[OT4]{fontenc} 
\usepackage{graphicx,color} %include pdf's (and png's for raster graphics... avoid raster graphics!) 
\usepackage{url} 
\usepackage[pdftex,hyperfootnotes=false,pdfborder={0 0 0}]{hyperref} %za wszystkimi pakietami; pdfborder nie wszedzie tak samo zaimplementowane bo specyfikacja nieprecyzyjna; pod miktex'em po prostu nie widac wtedy ramek


\input{_ustawienia.tex}

\begin{document}

\input{_tytulowa}

%%%%%%%%%%%%%%%%%% Część I %%%%%%%%%%%%%%%%%

\section{Generowanie drzew decyzyjnych}

\subsection{Generowanie drzewa}

\begin{itemize}
\item \textbf{Obejrzyj zawartość plików \emph{golf.nam}, \emph{golf.dat} i~\emph{golf.tst}; ile przykładów zawiera zbiór uczący? Iloma atrybutami są~one opisane?}
	\\Zbiór uczący zawiera 14 przykładów. Są one opisane pięcioma atrybutami, w tym jednym atrybutem decyzyjnym.

\item \textbf{Wygeneruj drzewo dla zbioru przykładów \emph{golf}; ustawienia standardowe.}
	 Unpruned decision tree:

\begin{verbatim}
outlook = overcast: Play (4.0)
outlook = sunny:
|   humidity <= 75 : Play (2.0)
|   humidity > 75 : Don't Play (3.0)
outlook = rain:
|   windy = true: Don't Play (2.0)
|   windy = false: Play (3.0)

Pruned decision tree:

outlook = overcast: Play (4.0/1.2)
outlook = sunny:
|   humidity <= 75 : Play (2.0/1.0)
|   humidity > 75 : Don't Play (3.0/1.1)
outlook = rain:
|   windy = true: Don't Play (2.0/1.0)
|   windy = false: Play (3.0/1.1)
\end{verbatim}



\item \textbf{Przeanalizuj wyniki; czy udało się przeprowadzić pruning?}
	\\Nie udało się przeprowadzić pruningu. Drzewa są identyczne.

\item \textbf{Obejrzyj drzewo; ile ma węzłów decyzyjnych, a~ile liści?}
	\\Wygenerowane drzewo ma 3 węzły decyzyjne i 5 liści.

\item \textbf{ Prześledź ścieżkę od korzenia do wybranego liścia.
}		\\W korzeniu znajduje się test związany z atrybutem outlook. Jeśli dla klasyfikowanego przykładu wartość atrybutu outlook wynosi "sunny", to następny węzeł na ścieżce to ten związany z atrybutem humidity. Zakłądając, że dla naszego przykładu wilgotność jest większ niż $ 75\% $ , liściem w analizowanej ścieżce będzie liść odnoszący się decyzji "Don't Play".
		\\Inaczej mówiąc, ścieżk ta klasyfikuje wszystkie dni w które jest słoneczna pogoda i wilgotność większa niż $ 75\% $ jako dni, w które nie gra się w golfa.
		
\item \textbf{Porównaj estymaty błędu dla drzewa oryginalnego (\emph{Unpruned}) i~uproszczonego (\emph{Pruned}).}
	\\Estymata błędu dla oryginalnego drzewa wynosi  $ 0\% $ a dla uproszczonego $ 38,5\% $ .

\item \textbf{Obejrzyj macierz pomyłek.}
	\\Ponieważ drzewo oryginalne nie generuje błędów dla zbioru uczącego to też jedyne co możemy z niego odczytać, to właśnie, że żadne obiekty nie są błędnie klasyfikowane, oraz że w przypadku 9 przykładów drzewo poprawnie zaklasyfikowało przykłądy do klasy "Play" a w 5 przypadkach do klasy "Don't Play".

\end{itemize}

\subsection{Konsultowanie}

\begin{itemize}
\item Dokonaj konsultacji wymyślonego przykładu dla wygenerowanego drzewa.
\item Konsultowanie przykładu ,,niepełnego''; dokładnie przeanalizuj wynik.
\item Konsultowanie, gdy znany jest rozkład prawdopodobieństwa.
\end{itemize}

\subsection{Różnica między \emph{gain ratio} a~\emph{info gain} w~praktyce}

\begin{itemize}
\item Obejrzyj zbiór \emph{testgain} (\emph{dat} i~\emph{nam}).
\item Wygeneruj dla niego dwukrotnie drzewo z~użyciem opcji \emph{gain ratio} i~\emph{info gain}; skomentuj wyniki.
\end{itemize}

\subsection{Grupowanie wartości atrybutów}

\begin{itemize}
\item Wygeneruj drzewo dla zbioru \emph{testgain}, zaznaczając opcję \emph{Subsetting}.
\item Analogicznie dla \emph{CRX}: opisać problem (przyznawanie kard kretytowych), obejrzeć zbiór (atrybuty \emph{A4}, \emph{A6} i~\emph{A7} mają wiele wartości); wygenerować drzewo bez i~z~grupowaniem.
\item Obejrzeć macierz pomyłek dla zbioru uczącego i~testującego; czy w~tym zastosowaniu przydałaby się macierz kosztów pomyłek?
\end{itemize}

\subsection{Poszukiwanie optymalnej wielkości drzewa uproszczonego}

\begin{itemize}
\item Poszukiwanie optymalnej wielkości drzewa uproszczonego przez dobór poziomu ufności procedury upraszczającej (\emph{Pruning confidence level}); przeprowadź serię eksperymentów \emph{10-fold cross-validation} dla zbioru \emph{Monk2}, ze~zmieniającym się poziomem ufności od~0.05 do~0.5, z~krokiem co najwyżej 0.05. Sporządź wykres zależności:

\begin{itemize}
\item średniego (po \emph{cross-validation}) rozmiaru drzewa uproszczonego,
\item średniej trafności klasyfikowania drzewa uproszczonego na~zbiorze testującym,
\item średniej estymaty błędu dla drzewa uproszczonego
\end{itemize}

...w funkcji poziomu ufności (odnieś te wyniki do średniej charakterystyki drzewa oryginalnego, nieuproszczonego).

\item Poszukiwanie optymalnej wielkości drzewa uproszczonego poprzez prepruning, tj.~manewrowanie minimalną licznością węzła (\emph{Minimum objects}). Dla zbioru \emph{CRX} przebadać przedział od~2 do~10.
\item Analiza wygenerowanego drzewa: poszukiwanie słabych punktów (liści o~małym wsparciu, poddrzew które generują szczególnie dużo błędów, etc.).
\end{itemize}

\subsection{\emph{Windowing}}

\begin{itemize}
\item Wyjaśnić zasadę i~opcje: \emph{Trials}, \emph{Initial window size}, \emph{Window increment}.
\item Analiza wyników (\emph{CRX}).
\end{itemize}

\subsection{Generowanie krzywej uczenia}

\begin{itemize}
\item Dla zbioru \emph{vote} przygotować kilka[naście] zbiorów uczących o~liczności $n$ zmieniającej się od~50 do~300, ze~skokiem np.~50 przypadków, poprzez wybieranie pierwszych $n$ ze~zbioru \emph{vote.dat}.
\item Wykreślić jako funkcję $n$ rozmiar drzewa uproszczonego oraz trafność klasyfikowania drzewa uproszczonego na zbiorze testującym.

\end{itemize}

\subsection{Maksymalizacja trafności}

\begin{itemize}
\item Uzyskaj jak najwyższą trafność klasyfikowania ze~zbioru \emph{GERMAN} w~eksperymencie \emph{10-fold CV}. Jakimi parametrami i~mechanizmami można manipulować, by~szukać najwyższej trafności? Kiedy można ufać tak uzyskanej trafności, a~kiedy można mówić o~nadużyciu?
\end{itemize}

%%%%%%%%%%%%%%%%%% Część II %%%%%%%%%%%%%%%%%

\section{Generowanie reguł decyzyjnych}

\subsection{Metoda pośrednia generowania reguł (\emph{C4.5rules})}

\begin{itemize}
\item Wygeneruj reguły dla zbioru \emph{GOLF} za~pomocą programu \emph{C4.5 for Windows}.


\begin{verbatim}
Rule 1: [70.7%]
    IF    outlook = overcast
    THEN  Play

Rule 2: [63.0%]
    IF    outlook = rain
    AND   windy = false
    THEN  Play

Rule 3: [63.0%]
    IF    outlook = sunny
    AND   humidity > 75
    THEN  Don't Play

Rule 4: [50.0%]
    IF    outlook = rain
    AND   windy = true
    THEN  Don't Play

Default class: Play

Errors in training set: 0 (0.0%)
\end{verbatim}

\item Porównaj wygenerowane reguły z~wyjściowym drzewem decyzyjnym. Czy reguły odzwierciedlają precyzyjnie drzewo?

\begin{verbatim}
Pruned decision tree:

outlook = overcast: Play (4.0/1.2)
outlook = sunny:
|   humidity <= 75 : Play (2.0/1.0)
|   humidity > 75 : Don't Play (3.0/1.1)
outlook = rain:
|   windy = true: Don't Play (2.0/1.0)
|   windy = false: Play (3.0/1.1)
\end{verbatim}

Powyższe drzewo nie może być odtworzone przy użyciu samych tylko uzyskanych reguł. Jest tak, ponieważ nie pokrywają one wszystkich możliwych ścieżek od~korzenia do liścia -- brakuje reguły dla \texttt{outlook = sunny AND humidity <= 75}. Ponieważ jednak w~wyniku zawarta jest również klasa domyślna, w~tym wypadku \texttt{Play}, możemy ją wykorzystać jako liść dla ścieżek w~drzewie odpowiadającym brakującym regułom. W~ten sposób, w~tym konkretnym przypadku, możliwe jest zrekonstruowanie drzewa. Stąd reguły odzwierciedlają drzewo w~sposób precyzyjny.
\end{itemize}

\subsection{Porównanie klasyfikowania za pomocą drzew decyzyjnych i~reguł decyzyjnych (\emph{C4.5rules})}

\begin{itemize}
\item Przeprowadź testy \emph{10-fold CV} na wybranych zbiorach dla drzew i~reguł.


\emph{Cross-validation} dla zbioru \emph{golf}:\\\\
\begin{tabular}{|r||r|rr|rr||r|rr|rr|r|}
\hline
&\multicolumn{5}{1||}{Before pruning}&\multicolumn{6}{1|}{After pruning} \\
\hline
Tree & 
Size & 
\multicolumn{2}{1|}{Errors} & 
\multicolumn{2}{1||}{Errors (test)} & 
Size & 
\multicolumn{2}{1|}{Errors} & 
\multicolumn{2}{1|}{Errors (test)} & 
Estimate \\
\hline\hline
    1 &    8 &    0 &  0.0\% &    0 &   0.0\% &    8 &    0 &  0.0\% &    0 &   0.0\% &  43.5\%  \\
    2 &    8 &    0 &  0.0\% &    1 &  50.0\% &    8 &    0 &  0.0\% &    1 &  50.0\% &  43.1\%  \\
    3 &    6 &    2 & 16.7\% &    1 &  50.0\% &    1 &    4 & 33.3\% &    1 &  50.0\% &  47.5\%  \\
    4 &    6 &    1 &  8.3\% &    1 &  50.0\% &    6 &    1 &  8.3\% &    1 &  50.0\% &  44.5\%  \\
    5 &    6 &    1 &  7.7\% &    1 & 100.0\% &    6 &    1 &  7.7\% &    1 & 100.0\% &  42.1\%  \\
    6 &    8 &    0 &  0.0\% &    0 &   0.0\% &    8 &    0 &  0.0\% &    0 &   0.0\% &  41.0\%  \\
    7 &    8 &    0 &  0.0\% &    0 &   0.0\% &    8 &    0 &  0.0\% &    0 &   0.0\% &  41.0\%  \\
    8 &    8 &    0 &  0.0\% &    0 &   0.0\% &    8 &    0 &  0.0\% &    0 &   0.0\% &  40.6\%  \\
    9 &    8 &    0 &  0.0\% &    0 &   0.0\% &    8 &    0 &  0.0\% &    0 &   0.0\% &  40.6\%  \\
   10 &    6 &    1 &  7.7\% &    1 & 100.0\% &    6 &    1 &  7.7\% &    1 & 100.0\% &  42.1\%  \\
\hline\hline
 Avg. &  7.2 &  0.5 &  4.0\% &  0.5 &  35.0\% &  6.7 &  0.7 &  5.7\% &  0.5 &  35.0\% &  42.6\%  \\
\hline
\end{tabular}

\begin{tabular}{|r|r|rr|rr|}
\hline
 Ruleset & 
 Size & 
 \multicolumn{2}{1|}{Errors} & 
 \multicolumn{2}{1|}{Errors (test)} \\
\hline\hline
       1 &    2 &    0 &  0.0\%  &    0 &   0.0\% \\
       2 &    3 &    0 &  0.0\%  &    1 &  50.0\% \\
       3 &    1 &    4 & 33.3\%  &    1 &  50.0\% \\
       4 &    3 &    1 &  8.3\%  &    2 & 100.0\% \\
       5 &    4 &    1 &  7.7\%  &    1 & 100.0\% \\
       6 &    4 &    0 &  0.0\%  &    0 &   0.0\% \\
       7 &    4 &    0 &  0.0\%  &    0 &   0.0\% \\
       8 &    3 &    0 &  0.0\%  &    0 &   0.0\% \\
       9 &    3 &    0 &  0.0\%  &    0 &   0.0\% \\
      10 &    2 &    1 &  7.7\%  &    1 & 100.0\% \\
\hline\hline
    Avg. &  2.9 &  0.7 &  5.7\%  &  0.6 &  40.0\% \\
\hline
\end{tabular}


\emph{Cross-validation} dla zbioru \emph{vote}:\\\\
\begin{tabular}{|r||r|rr|rr||r|rr|rr|r|}
\hline
&\multicolumn{5}{1||}{Before pruning}&\multicolumn{6}{1|}{After pruning} \\
\hline
Tree & 
Size & 
\multicolumn{2}{1|}{Errors} & 
\multicolumn{2}{1||}{Errors (test)} & 
Size & 
\multicolumn{2}{1|}{Errors} & 
\multicolumn{2}{1|}{Errors (test)} & 
Estimate \\
\hline\hline
    1 &   25 &    7 & 2.6\% &    4 & 13.3\% &    7 &   12 & 4.4\% &    1 &  3.3\%  &   7.3\%  \\
    2 &   25 &    7 & 2.6\% &    1 &  3.3\% &    7 &   12 & 4.4\% &    1 &  3.3\%  &   7.2\%  \\
    3 &   16 &    9 & 3.3\% &    0 &  0.0\% &    7 &   13 & 4.8\% &    0 &  0.0\%  &   7.7\%  \\
    4 &   19 &   10 & 3.7\% &    1 &  3.3\% &    7 &   13 & 4.8\% &    0 &  0.0\%  &   7.7\%  \\
    5 &   16 &    8 & 3.0\% &    1 &  3.3\% &    7 &   11 & 4.1\% &    2 &  6.7\%  &   6.9\%  \\
    6 &   16 &    8 & 3.0\% &    4 & 13.3\% &    7 &   11 & 4.1\% &    2 &  6.7\%  &   6.9\%  \\
    7 &   31 &    5 & 1.9\% &    4 & 13.3\% &    4 &   13 & 4.8\% &    2 &  6.7\%  &   7.0\%  \\
    8 &   28 &    5 & 1.9\% &    4 & 13.3\% &    4 &   12 & 4.4\% &    3 & 10.0\%  &   6.6\%  \\
    9 &   16 &    7 & 2.6\% &    2 &  6.7\% &    7 &   11 & 4.1\% &    2 &  6.7\%  &   6.8\%  \\
   10 &   13 &    8 & 3.0\% &    3 & 10.0\% &    7 &   11 & 4.1\% &    2 &  6.7\%  &   6.8\%  \\
\hline\hline
 Avg. & 20.5 &  7.4 & 2.8\% &  2.4 & 8.0\%  &  6.4 & 11.9 & 4.4\% &  1.5 &  5.0\%  &   7.1\%  \\
\hline
\end{tabular}

\begin{tabular}{|r|r|rr|rr|}
\hline
 Ruleset & 
 Size & 
 \multicolumn{2}{1|}{Errors} & 
 \multicolumn{2}{1|}{Errors (test)} \\
\hline\hline
       1 &    4 &   12 & 4.4\% &    1 &  3.3\% \\
       2 &    3 &   12 & 4.4\% &    1 &  3.3\% \\
       3 &    5 &   11 & 4.1\% &    0 &  0.0\% \\
       4 &    4 &   13 & 4.8\% &    0 &  0.0\% \\
       5 &    5 &    9 & 3.3\% &    2 &  6.7\% \\
       6 &    4 &   11 & 4.1\% &    2 &  6.7\% \\
       7 &    4 &    7 & 2.6\% &    2 &  6.7\% \\
       8 &    4 &    9 & 3.3\% &    4 & 13.3\% \\
       9 &    4 &    9 & 3.3\% &    2 &  6.7\% \\
      10 &    5 &    8 & 3.0\% &    3 & 10.0\% \\
\hline\hline
    Avg. &  4.2 & 10.1 & 3.7\% &  1.7 &  5.7\% \\
\hline
\end{tabular}

\item Porównaj wyniki pod kątem trafności klasyfikowania na zbiorze testującym oraz rozmiaru opisu.
\item Przeprowadzając kilka eksperymentów uczenia i~testowania przeanalizuj wpływ parametrów \emph{Confidence Level} i~\emph{Redundancy Factor} na~otrzymywany zbiór reguł.
\end{itemize}

\subsection{Generowanie reguł z~użyciem algorytmu \emph{LEM}}

\begin{itemize}
\item Wygeneruj reguły dla zbioru \emph{HPAP.ISF}.
\item Przyjrzyj się regułom możliwym; opisz je i~,,wydedukuj'', skąd się wzięły.
\end{itemize}

\subsection{Porównanie reguł generowanych za~pomocą algorytmu \emph{LEM} i~\emph{C4.5}}

\begin{itemize}
\item Wygeneruj reguły przy użyciu obu podejść dla zbiorów: \emph{HPAP}, \emph{VOTE} i~\emph{MONK}.
\item Przyjrzyj się niezależnie regułom pewnym i~możliwym (\emph{LEM}).
\end{itemize}

%Dla chętnych:
%\subsection{Przeprowadź eksperyment generowania i~testowania reguł LEM w~ramach cross validation}

%%%%%%%%%%%%%%%% literatura %%%%%%%%%%%%%%%%

\bibliography{sprawozd}
\bibliographystyle{plain}


\end{document}

